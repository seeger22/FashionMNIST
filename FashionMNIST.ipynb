{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TmyxtRDefqRe"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data and normalize\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))]) # change color to greyscale\n",
        "\n",
        "train = datasets.FashionMNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "train_load = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
        "\n",
        "test = datasets.FashionMNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
        "test_load = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "oXymFMEOha3h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define neural net\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# nn with all linear layers, Relu activation, dropout regularization, softmax for output\n",
        "model = nn.Sequential(nn.Linear(784, 256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Dropout(0.2),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim = 1)\n",
        "                     )\n",
        "\n",
        "# loss function, cross entropy\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# optimizer function w learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.002)\n",
        "\n",
        "# run 30 epochs\n",
        "epochs = 30\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in train_load:\n",
        "    # Flatten Fashion-MNIST images into a 784 long vector\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    \n",
        "    # Training pass\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    \n",
        "    # Turn off gradients for validation, saves memory and computation\n",
        "    with torch.no_grad():\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Validation pass\n",
        "      for images, labels in test_load:\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        log_ps = model(images)\n",
        "        test_loss += criterion(log_ps, labels)\n",
        "        \n",
        "        ps = torch.exp(log_ps)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "    model.train()\n",
        "    train_losses.append(running_loss/len(train_load))\n",
        "    test_losses.append(test_loss/len(test_load))\n",
        "    \n",
        "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(train_load)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(test_load)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(test_load)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNgx8ykk8hGK",
        "outputId": "17c2db2e-9c66-4470-ab08-1732a205702f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30.. Training loss: 0.595.. Test loss: 0.473.. Test Accuracy: 0.828\n",
            "Epoch: 2/30.. Training loss: 0.451.. Test loss: 0.435.. Test Accuracy: 0.845\n",
            "Epoch: 3/30.. Training loss: 0.424.. Test loss: 0.401.. Test Accuracy: 0.858\n",
            "Epoch: 4/30.. Training loss: 0.398.. Test loss: 0.394.. Test Accuracy: 0.859\n",
            "Epoch: 5/30.. Training loss: 0.384.. Test loss: 0.374.. Test Accuracy: 0.865\n",
            "Epoch: 6/30.. Training loss: 0.377.. Test loss: 0.365.. Test Accuracy: 0.866\n",
            "Epoch: 7/30.. Training loss: 0.363.. Test loss: 0.366.. Test Accuracy: 0.867\n",
            "Epoch: 8/30.. Training loss: 0.352.. Test loss: 0.358.. Test Accuracy: 0.870\n",
            "Epoch: 9/30.. Training loss: 0.347.. Test loss: 0.377.. Test Accuracy: 0.868\n",
            "Epoch: 10/30.. Training loss: 0.343.. Test loss: 0.365.. Test Accuracy: 0.875\n",
            "Epoch: 11/30.. Training loss: 0.340.. Test loss: 0.368.. Test Accuracy: 0.871\n",
            "Epoch: 12/30.. Training loss: 0.333.. Test loss: 0.353.. Test Accuracy: 0.878\n",
            "Epoch: 13/30.. Training loss: 0.329.. Test loss: 0.351.. Test Accuracy: 0.875\n",
            "Epoch: 14/30.. Training loss: 0.326.. Test loss: 0.379.. Test Accuracy: 0.871\n",
            "Epoch: 15/30.. Training loss: 0.328.. Test loss: 0.348.. Test Accuracy: 0.879\n",
            "Epoch: 16/30.. Training loss: 0.316.. Test loss: 0.350.. Test Accuracy: 0.878\n",
            "Epoch: 17/30.. Training loss: 0.317.. Test loss: 0.369.. Test Accuracy: 0.876\n",
            "Epoch: 18/30.. Training loss: 0.314.. Test loss: 0.367.. Test Accuracy: 0.875\n",
            "Epoch: 19/30.. Training loss: 0.309.. Test loss: 0.345.. Test Accuracy: 0.880\n",
            "Epoch: 20/30.. Training loss: 0.302.. Test loss: 0.381.. Test Accuracy: 0.873\n",
            "Epoch: 21/30.. Training loss: 0.306.. Test loss: 0.361.. Test Accuracy: 0.877\n",
            "Epoch: 22/30.. Training loss: 0.303.. Test loss: 0.354.. Test Accuracy: 0.886\n",
            "Epoch: 23/30.. Training loss: 0.300.. Test loss: 0.344.. Test Accuracy: 0.882\n",
            "Epoch: 24/30.. Training loss: 0.293.. Test loss: 0.351.. Test Accuracy: 0.880\n",
            "Epoch: 25/30.. Training loss: 0.295.. Test loss: 0.349.. Test Accuracy: 0.886\n",
            "Epoch: 26/30.. Training loss: 0.291.. Test loss: 0.347.. Test Accuracy: 0.883\n",
            "Epoch: 27/30.. Training loss: 0.293.. Test loss: 0.342.. Test Accuracy: 0.884\n",
            "Epoch: 28/30.. Training loss: 0.284.. Test loss: 0.357.. Test Accuracy: 0.881\n",
            "Epoch: 29/30.. Training loss: 0.288.. Test loss: 0.382.. Test Accuracy: 0.875\n",
            "Epoch: 30/30.. Training loss: 0.291.. Test loss: 0.359.. Test Accuracy: 0.882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best results at epoch 22 and 25 based on accuracy"
      ],
      "metadata": {
        "id": "xSyphAVKDHB8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}